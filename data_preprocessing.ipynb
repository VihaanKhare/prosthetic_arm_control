{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ceb9bb93-d557-4551-b51a-96b028f1e844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: EMG-data.csv\n",
      "Original total samples: 4237907\n",
      "Limiting to 50000 samples per class...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aman Prajapati\\AppData\\Local\\Temp\\ipykernel_2184\\3294324419.py:94: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_limited = df.groupby(label_column).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New total samples after limiting: 363696\n",
      "Class distribution in new dataset:\n",
      "class\n",
      "0    50000\n",
      "1    50000\n",
      "2    50000\n",
      "3    50000\n",
      "4    50000\n",
      "5    50000\n",
      "6    50000\n",
      "7    13696\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Applying Bandpass Filter...\n",
      "Normalizing Data (Z-score)...\n",
      "Segmenting data into 200ms windows with 100ms overlap...\n",
      "\n",
      "--- Final Preprocessing Results ---\n",
      "Final Time Windows (X.npy) shape: (3635, 200, 8)\n",
      "Final Labels (y.npy) shape: (3635,)\n",
      "Saving X.npy and y.npy to disk...\n",
      "‚úÖ Preprocessing complete. Small, class-balanced dataset saved.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import butter, lfilter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from typing import Tuple\n",
    "import os\n",
    "\n",
    "# --- 1. CONFIGURATION CONSTANTS ---\n",
    "# Use the same constants derived from the previous step (assuming Fs=1000Hz)\n",
    "FS = 1000             # Sampling frequency in Hz\n",
    "LOW_CUTOFF = 20       # Bandpass low cutoff in Hz\n",
    "HIGH_CUTOFF = 450     # Bandpass high cutoff in Hz\n",
    "FILTER_ORDER = 4      # Order of the Butterworth filter\n",
    "\n",
    "WINDOW_SIZE_MS = 200  # Time window for segmentation (e.g., 200 ms)\n",
    "OVERLAP_MS = 100      # Overlap between consecutive windows (e.g., 100 ms)\n",
    "\n",
    "# üÜï NEW: Set the maximum number of samples (rows) you want per gesture class.\n",
    "# This will drastically reduce the overall dataset size while keeping class balance.\n",
    "MAX_SAMPLES_PER_CLASS = 50000 # Example: Limit to 50,000 samples per class (50,000 * 8 classes = 400,000 total samples)\n",
    "\n",
    "# --- 2. CORE UTILITY FUNCTIONS (Retained from previous step) ---\n",
    "\n",
    "def butter_bandpass(lowcut: float, highcut: float, fs: float, order: int=4) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Generates the Butterworth filter coefficients (b, a).\"\"\"\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "def apply_bandpass_filter(data: np.ndarray, lowcut: float, highcut: float, fs: float, order: int=4) -> np.ndarray:\n",
    "    \"\"\"Applies the Bandpass filter to all channels of the EMG data.\"\"\"\n",
    "    # ... implementation (same as before) ...\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    filtered_data = np.zeros_like(data, dtype=np.float64)\n",
    "    for i in range(data.shape[1]):\n",
    "        filtered_data[:, i] = lfilter(b, a, data[:, i])\n",
    "    return filtered_data\n",
    "\n",
    "def normalize_data(data: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Applies Z-score (StandardScaler) normalization to the data.\"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    return scaler.fit_transform(data)\n",
    "\n",
    "def segment_data(X: np.ndarray, y: np.ndarray, fs: float, window_size_ms: int, overlap_ms: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Segments the time-series data into overlapping windows.\"\"\"\n",
    "    # ... implementation (same as before) ...\n",
    "    window_samples = int(fs * (window_size_ms / 1000.0))\n",
    "    overlap_samples = int(fs * (overlap_ms / 1000.0))\n",
    "    stride = window_samples - overlap_samples\n",
    "\n",
    "    X_segments, y_segments = [], []\n",
    "\n",
    "    if window_samples <= 0 or stride <= 0 or window_samples > len(X):\n",
    "         raise ValueError(\"Invalid window/overlap configuration.\")\n",
    "\n",
    "    for i in range(0, len(X) - window_samples + 1, stride):\n",
    "        segment_X = X[i : i + window_samples, :]\n",
    "        X_segments.append(segment_X)\n",
    "\n",
    "        segment_y = y[i : i + window_samples]\n",
    "        unique, counts = np.unique(segment_y, return_counts=True)\n",
    "        y_segments.append(unique[np.argmax(counts)])\n",
    "\n",
    "    X_final = np.array(X_segments, dtype=np.float32)\n",
    "    y_final = np.array(y_segments, dtype=np.int32)\n",
    "    return X_final, y_final\n",
    "\n",
    "# --- 3. DATA LOADING AND SAMPLING FUNCTIONS ---\n",
    "\n",
    "def load_emg_data(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Loads the entire dataset into a DataFrame.\"\"\"\n",
    "    print(f\"Loading data from: {file_path}\")\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Error: File not found at {file_path}. Please check the path.\")\n",
    "    \n",
    "    df = pd.read_csv(file_path)\n",
    "    return df\n",
    "\n",
    "def limit_data_by_class_count(df: pd.DataFrame, max_samples_per_class: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Limits the number of samples for each class to max_samples_per_class\n",
    "    to create a smaller, class-balanced dataset.\n",
    "    \"\"\"\n",
    "    label_column = 'class'\n",
    "    emg_columns = [f'channel{i}' for i in range(1, 9)]\n",
    "    \n",
    "    print(f\"Original total samples: {len(df)}\")\n",
    "    print(f\"Limiting to {max_samples_per_class} samples per class...\")\n",
    "    \n",
    "    # Group by the 'class' label and take a sample of up to max_samples_per_class\n",
    "    # .sample() with replace=False (default) will take all samples if count is larger than group size.\n",
    "    df_limited = df.groupby(label_column).apply(\n",
    "        lambda x: x.sample(min(len(x), max_samples_per_class), random_state=42)\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    X_raw = df_limited[emg_columns].values\n",
    "    y_raw = df_limited[label_column].values \n",
    "\n",
    "    print(f\"New total samples after limiting: {len(df_limited)}\")\n",
    "    print(f\"Class distribution in new dataset:\\n{df_limited[label_column].value_counts().sort_index()}\")\n",
    "    \n",
    "    return X_raw, y_raw\n",
    "\n",
    "# --- 4. MAIN EXECUTION ---\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    KAGGLE_FILE_PATH = 'EMG-data.csv'\n",
    "\n",
    "    try:\n",
    "        # 1. Load Data (Load the full, large DataFrame first)\n",
    "        df_full = load_emg_data(KAGGLE_FILE_PATH)\n",
    "\n",
    "        # 2. Limit and Balance Data (Crucial step for small model training) ü§è\n",
    "        X_raw, y_raw = limit_data_by_class_count(df_full, MAX_SAMPLES_PER_CLASS)\n",
    "        del df_full # Free up memory\n",
    "\n",
    "        # --- Data Preprocessing Pipeline (Applied to the smaller subset) ---\n",
    "\n",
    "        # 3. Bandpass Filter (20-450 Hz)\n",
    "        print(\"\\nApplying Bandpass Filter...\")\n",
    "        X_filtered = apply_bandpass_filter(X_raw, LOW_CUTOFF, HIGH_CUTOFF, FS, FILTER_ORDER)\n",
    "\n",
    "        # 4. Normalize Data\n",
    "        print(\"Normalizing Data (Z-score)...\")\n",
    "        X_normalized = normalize_data(X_filtered)\n",
    "\n",
    "        # 5. Segment Data into Time Windows\n",
    "        print(f\"Segmenting data into {WINDOW_SIZE_MS}ms windows with {OVERLAP_MS}ms overlap...\")\n",
    "        X_final, y_final = segment_data(X_normalized, y_raw, FS, WINDOW_SIZE_MS, OVERLAP_MS)\n",
    "\n",
    "        print(\"\\n--- Final Preprocessing Results ---\")\n",
    "        print(f\"Final Time Windows (X.npy) shape: {X_final.shape}\")\n",
    "        print(f\"Final Labels (y.npy) shape: {y_final.shape}\")\n",
    "\n",
    "        # 6. Deliverables: Save Final Arrays\n",
    "        print(\"Saving X.npy and y.npy to disk...\")\n",
    "        np.save('X.npy', X_final)\n",
    "        np.save('y.npy', y_final)\n",
    "        print(\"‚úÖ Preprocessing complete. Small, class-balanced dataset saved.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Data pipeline failed. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e2e9283-2ab8-4cbd-a178-929a7404a61b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0096a45-cf97-4793-bfdf-034ff8d0cf6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
